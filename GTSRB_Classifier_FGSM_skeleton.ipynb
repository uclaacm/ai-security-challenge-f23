{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVJ1QIw23Ve7"
      },
      "source": [
        "# German Traffic Sign Recognition Benchmark (GTSRB) Classifier\n",
        "\n",
        "We will use the [German Traffic Sign Recognition Benchmark (GTSRB)](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset) dataset to train a classifier to recognize traffic signs. \n",
        "\n",
        "GTSRB is a multi-class, single-image classification challenge. Our goal is to correctly identify traffic signs from their dataset containing more than 50,000 images of 43 classes.\n",
        "\n",
        "## Fast Gradient Sign Method (FGSM) Attack\n",
        "\n",
        "This notebook will introduce you to the Fast Gradient Sign Method (FGSM) attack and help you understand how to implement it. \\\n",
        "\n",
        "*Please make a copy of this notebook before you get started so that your work can be saved!!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q82TLyN3Ve_"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqnLiNQB3Ve_"
      },
      "outputs": [],
      "source": [
        "# py(torch): our machine learning library!\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "# Pillow: Python Image Library\n",
        "from PIL import Image\n",
        "# pandas: data analysis library\n",
        "import pandas as pd\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch uses CPU by default. The following cell checks whether you have GPU available and if so, uses GPU. \\\n",
        "***For people doing local setup** If you want to use GPU, you need to install CUDA and CuDNN from NVIDIA and use the following cell to set the device to GPU.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYxBChis3VfB",
        "outputId": "213774f3-220f-4a4e-9b8d-a80d2bb806e6"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")  # check if we are on CPU or GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3E9i7kU3xNC"
      },
      "source": [
        "### Mount drive\n",
        "\n",
        "uncomment the following lines to mount drive if you need to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub0DGYeHqW0G",
        "outputId": "a98fe044-614a-4e17-e611-a28cf4c7e719"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVlqXNEtquGi"
      },
      "source": [
        "## Get the training and test data\n",
        "\n",
        "We will download the training and test data from the GTSRB website. \n",
        "\n",
        "The training data is a zip file that contains 43 subdirectories (one for each class) containing the images. \\\n",
        "The test data contains a single directory with the images, and it is for some reason unlabeled, so we will need to download the label contained in a csv file. \n",
        "\n",
        "We will use the `wget` command to download the data and the `unzip` command to unzip the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zUMv5HoAUDzh",
        "outputId": "82ea6ca7-0224-4684-b684-a7cfc2476472"
      },
      "outputs": [],
      "source": [
        "!pip install wget  # wget: for downloading model weights\n",
        "import wget\n",
        "\n",
        "# train data\n",
        "train_data_url = \"https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip\"\n",
        "wget.download(train_data_url)\n",
        "\n",
        "# test data\n",
        "test_data_url = \"https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip\"\n",
        "wget.download(test_data_url)\n",
        "\n",
        "# the test data comes unlabeled so we need to download the labels as well\n",
        "test_label_url = \"https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip\"\n",
        "wget.download(test_label_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkCcbk6gqsi6",
        "outputId": "9c7e851f-026a-474f-919a-081db0b9d54a"
      },
      "outputs": [],
      "source": [
        "!unzip -u \"/content/GTSRB_Final_Training_Images.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JyRhIelqtBu",
        "outputId": "8bae9692-ca7b-4225-bd10-8a25e1e579c8"
      },
      "outputs": [],
      "source": [
        "!unzip -u \"/content/GTSRB_Final_Test_Images.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOsKynfUscDi",
        "outputId": "664ba4ca-3a96-45e0-b0be-a6d78b3e8880"
      },
      "outputs": [],
      "source": [
        "!unzip -u \"/content/GTSRB_Final_Test_GT.zip\" -d \"/content/GTSRB/Final_Test/Images\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1pOHYbv3VfB"
      },
      "source": [
        "### Load Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You may notice that the images are in .ppm format, which you're probably not familiar with. But don't worry! Using `PIL`, the Python Image Library, we can process them just fine.\n",
        "\n",
        "We need to first **resize** the images because they come in dimensions that vary between 15x15 to 250x250 pixels. AND they are not necessarily square. We will resize them to 224x224 pixels because thats what's commonly used in image classification tasks. \n",
        "\n",
        "We will also **normalize** the images so that the pixel values are between 0 and 1, which is easier for our model to process.\n",
        "\n",
        "`Dataloader` is a class that helps us load the data in batches. We will use it to load the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "U39AAkCg3VfB",
        "outputId": "c32f3cfd-bcdd-426e-b599-adb726eb8475"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = \"GTSRB/Final_Training/Images\"\n",
        "\n",
        "# normalize images in the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# we create a dataset object with the images in the training folder, and with the dataset we create a dataloader\n",
        "train_dataset = datasets.ImageFolder(TRAIN_PATH, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4loQPp0l3VfB"
      },
      "source": [
        "### Load test data\n",
        "\n",
        "We need a separate `CustomTestDataset` function to load the test data because it is not in the same format as the training data where the images are in separate folders according to their label.\n",
        "\n",
        "The test data is in a single folder and the labels are in a separate csv file. We will use the `csv` library to read the csv file and the `PIL` library to process the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYArxSM-3VfC",
        "outputId": "73774bc9-78e0-4131-be33-8d6edb19ffda"
      },
      "outputs": [],
      "source": [
        "TEST_PATH = \"GTSRB/Final_Test/Images/\"\n",
        "CSV_PATH = \"GTSRB/Final_Test/Images/GT-final_test.csv\"\n",
        "class CustomTestDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        # read csv file, seperator is ';'\n",
        "        self.dataframe = pd.read_csv(csv_file, sep=';')\n",
        "        # look at columns of dataframe\n",
        "        print(self.dataframe.columns)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.dataframe.iloc[idx, 0])\n",
        "        image = Image.open(img_name)\n",
        "        label = int(self.dataframe.iloc[idx, -1])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# normalize images in the dataset\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "test_dataset = CustomTestDataset(csv_file=CSV_PATH,\n",
        "                                 img_dir=TEST_PATH,\n",
        "                                 transform=test_transform)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfCeVgct3VfD"
      },
      "source": [
        "## Training and Testing\n",
        "\n",
        "This is your typical train and test functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5fO7OkF3VfD"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device, epochs=10, print_every=200):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs = data[0].to(device)\n",
        "            labels = data[1].to(device)  # Move data to GPU\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "        if epoch % print_every == print_every - 1:\n",
        "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / print_every:.3f}\")\n",
        "            running_loss = 0.0\n",
        "    return total_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aopLBI5s3VfD"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)  # Move data to GPU\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxqY_i5z3VfE"
      },
      "source": [
        "## Define Model Architecture\n",
        "\n",
        "*Skip to the next section if you want to use our pretrained model*\n",
        "\n",
        "We are using **transfer learning** to train our model. For our base model, we use a pretrained ResNet18 model and replace the last layer with a fully connected layer with 43 outputs (one for each class).\n",
        "\n",
        "### ResNet18\n",
        "\n",
        "ResNet18 is a convolutional neural network that is 18 layers deep. It is trained on the ImageNet dataset which contains 1.2 million images with 1000 classes. The original ResNet18 model has 1000 outputs, but we only have 43 classes, hence we need to replace the last layer with a fully connected layer with 43 outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO8oeoKx3VfE",
        "outputId": "8474887a-6b6f-451d-ec1e-9f1ef456b4a4"
      },
      "outputs": [],
      "source": [
        "# We use a pretrained ResNet18 model and finetune it for our task\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 43)  # GTSRB has 43 classes so we need to change the last layer\n",
        "model = model.to(device) # move model to GPU if available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQhV7Uxg3VfE"
      },
      "source": [
        "### Training and Testing Loop\n",
        "\n",
        "We will use the `CrossEntropyLoss` function to calculate the loss and `SGD` (Stochastic Gradient Descent) optimizer to optimize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Klbapp423VfE"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, device, epochs=10, print_every=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHNxaL_h3VfE",
        "outputId": "080220f9-d817-4e63-b939-4ef3a0ded9d8"
      },
      "outputs": [],
      "source": [
        "evaluate(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJdVC_7D3VfF"
      },
      "source": [
        "## Save model and weights\n",
        "\n",
        "We are on a runtime, meaning that when we close the notebook, we will lose all are variables, including our model. \n",
        "So to make sure you don't lose your work, we will save the model and the weights. You typically only need one of them, but we will save both just in case.\n",
        "\n",
        "***Colab users look here:** you will need to download the model and weights manually from the files tab on the left.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBNMyobG3VfF"
      },
      "outputs": [],
      "source": [
        "# save the weights\n",
        "torch.save(model.state_dict(), 'initial_model_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDLLZCZV3VfF"
      },
      "outputs": [],
      "source": [
        "# save the entire model\n",
        "torch.save(model, 'initial_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BuJ2Ht63VfF"
      },
      "source": [
        "## Loading pretrained model\n",
        "\n",
        "The pretrained model is a model finetuned from the ResNet18 model with the last layer replaced with a linear layer with 43 outputs.\n",
        "Its trained on the GTSRB dataset and achieves a validation accuracy of 98.8%.\n",
        "\n",
        "We will use `wget` to download the model and weights from our github repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsnn5Xzj4mYh",
        "outputId": "e005ea4e-efea-4f9e-8fad-6117934906b5"
      },
      "outputs": [],
      "source": [
        "!pip install wget  # wget: for downloading model weights\n",
        "import wget\n",
        "\n",
        "# I have pre-trained a CNN model (with the architecture above) and saved the weights to the below\n",
        "# GitHub link, and we can load it using wget\n",
        "model_file = wget.download(\"https://github.com/LiptonJumboTeaBag/GTSRB-Classifier/blob/0e4ee766e25d2e787aa1382f08d4e1b710da91f9/initial_model.pth?raw=true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can load the model and weights using `torch.load()` and `model.load_state_dict()`.\n",
        "\n",
        "`torch.load()` loads the model and weights into a dictionary. \\\n",
        "`model.load_state_dict()` only loads the weights into the model, so you would need to predefine the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeS5QodO3VfF",
        "outputId": "107d38c0-adb4-4585-c7fe-25f7cad6c9dc"
      },
      "outputs": [],
      "source": [
        "model = torch.load(model_file)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT8oqWOK3VfF"
      },
      "source": [
        "### Sanity check\n",
        "\n",
        "We will do a sanity check to make sure the model is working as expected.\n",
        "Feel free to skip this section if you want to get to the attack or if you are on a CPU as it might take a while to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "9nY-zfVP3VfG",
        "outputId": "44bd2675-898f-4c90-f182-f662c4ff071a"
      },
      "outputs": [],
      "source": [
        "print(f'Accuracy: {100*(evaluate(model=model, test_loader=test_loader)):.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfWG6Y2p3VfG"
      },
      "source": [
        "## Visualizing our dataset\n",
        "\n",
        "To get a better understanding of our dataset, we will visualize it using the `matplotlib` library.\n",
        "\n",
        "The following code will display 1 image from each class in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pczqso1s3VfG",
        "outputId": "1ee3772f-ab18-49c9-fc4b-4c8aa780786b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Initialize a dictionary to hold one sample per class\n",
        "class_samples = {}\n",
        "\n",
        "# Function to unnormalize and convert tensor to numpy for plotting\n",
        "def imshow(img_tensor):\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    # unnormalize\n",
        "    for t, m, s in zip(img_tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "\n",
        "    npimg = img_tensor.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# Collect one sample per class from the training set\n",
        "for images, labels in train_loader:\n",
        "    for i in range(len(labels)):\n",
        "        label = labels[i].item()\n",
        "        if label not in class_samples:\n",
        "            class_samples[label] = images[i]\n",
        "    if len(class_samples) == 43:  # Assuming there are 43 classes in GTSRB\n",
        "        break\n",
        "\n",
        "# sort the dictionary by keys\n",
        "class_samples = dict(sorted(class_samples.items()))\n",
        "\n",
        "# Plot one image from each class\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "for i, (label, img) in enumerate(class_samples.items()):\n",
        "    ax = fig.add_subplot(7, 7, i + 1)  # Assuming a grid size that can fit all classes\n",
        "    ax.set_title(f\"Class {label}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # process the image and show it\n",
        "    imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYmGEXhJ3VfG"
      },
      "source": [
        "# FGSM Attack!\n",
        "\n",
        "Fast Gradient Sign Method (FGSM) is a white box attack that uses the gradients of the loss function to create an adversarial example. FGSM is a fast way to generate adversarial examples but it is not very strong.\n",
        "\n",
        "### FGSM Implementation\n",
        "\n",
        "We will implement FGSM using the following formula: \\\n",
        "`adversarial_image = original_image + epsilon * sign(gradient)`\n",
        "where epsilon is a hyperparameter that controls the magnitude of the perturbation.\n",
        "\n",
        "Learn more about FGSM [here](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm#:~:text=The%20fast%20gradient%20sign%20method%20works%20by%20using%20the%20gradients,is%20called%20the%20adversarial%20image.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7qYiqdc3VfG"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def adversarial_attack(image, model, epsilon):\n",
        "    # TODO: 1. Get the gradients of the loss w.r.t the input image\n",
        "    pass\n",
        "\n",
        "    # TODO: 2. Add the gradients to the input image\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnG6WKA33VfG"
      },
      "source": [
        "## Run fgsm attack on random image from test set\n",
        "\n",
        "We will run the fgsm attack on a random image from the test set and see how our model performs on the adversarial image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQFM6BmZ3VfH",
        "outputId": "c427be7a-1f4a-4584-809e-ad61a8f9d26e"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Load a random image from the test loader\n",
        "test_iter = iter(test_loader)\n",
        "images, labels = next(test_iter)\n",
        "idx = random.randint(0, len(labels) - 1)\n",
        "image, label = images[idx:idx+1], labels[idx:idx+1]\n",
        "image, label = image.to(device), label.to(device)\n",
        "\n",
        "# Print the ground truth label\n",
        "print(f\"Ground truth class label: {label.item()}\")\n",
        "\n",
        "# TODO: Experiment with different epsilon values\n",
        "epsilons = [0, 0.1, 0.5]\n",
        "ep_count = len(epsilons)\n",
        "\n",
        "# Display original and perturbed images\n",
        "PLOT_WIDTH = ep_count//2 + 1\n",
        "PLOT_HEIGHT = 2\n",
        "\n",
        "plt.figure(figsize=(PLOT_WIDTH*5, PLOT_HEIGHT*5))\n",
        "\n",
        "# Run FGSM attack with different epsilon values\n",
        "for i, eps in enumerate(epsilons):\n",
        "    # Generate perturbed image\n",
        "    perturbed_image = adversarial_attack(image, model, eps)\n",
        "    # Classify the perturbed image\n",
        "    perturbed_output = model(perturbed_image)\n",
        "    _, perturbed_pred = perturbed_output.max(1)\n",
        "\n",
        "    # Get confidence of the perturbed image using softmax\n",
        "    confidence = F.softmax(perturbed_output, dim=1)[0, perturbed_pred].item()\n",
        "\n",
        "\n",
        "    # Convert tensors to NumPy arrays for visualization\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    # we detach the image from the graph and move it to the CPU\n",
        "    perturbed_image = perturbed_image.detach()\n",
        "\n",
        "    # unnormalize\n",
        "    for t, m, s in zip( perturbed_image, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "\n",
        "    perturbed_image = perturbed_image.squeeze().cpu().numpy() # remove batch dimension and move to CPU\n",
        "\n",
        "    perturbed_image = perturbed_image.clip(0,1).transpose((1, 2, 0)) # clip to [0,1] and move channel to last dimension\n",
        "\n",
        "    plt.subplot(PLOT_HEIGHT, PLOT_WIDTH, i + 1)\n",
        "    plt.title(f\"Eps {eps}: Classified as {perturbed_pred.item()} with {confidence:.2f} confidence\")\n",
        "    plt.imshow(perturbed_image)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOiTphuU3VfH"
      },
      "source": [
        "## Run fgsm attack on YOUR image!!!\n",
        "\n",
        "Here you can upload your own image and run the fgsm attack on it!\n",
        "Replace the `image path` with your own image path and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU_G9bJl3VfH",
        "outputId": "a2d2b24b-a5fd-4992-db1c-1afea616f9bd"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the image\n",
        "image_path = \"PATH_TO_UR_IMAGE.smth\"  # Replace with the path to your image\n",
        "\n",
        "# To ensure the image is loaded correctly, we use the same preprocessing as we used for the training data\n",
        "input_image = Image.open(image_path)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
        "image = input_batch.to(device)\n",
        "\n",
        "epsilons = [0.1]\n",
        "ep_count = len(epsilons)\n",
        "\n",
        "# Display original and perturbed images\n",
        "PLOT_WIDTH = ep_count//2 + 1\n",
        "PLOT_HEIGHT = 2\n",
        "\n",
        "plt.figure(figsize=(PLOT_WIDTH*5, PLOT_HEIGHT*5))\n",
        "\n",
        "# Run FGSM attack with different epsilon values\n",
        "for i, eps in enumerate(epsilons):\n",
        "    # Generate perturbed image\n",
        "    perturbed_image = adversarial_attack(image, model, eps)\n",
        "    # Classify the perturbed image\n",
        "    perturbed_output = model(perturbed_image)\n",
        "    _, perturbed_pred = perturbed_output.max(1)\n",
        "    confidence = F.softmax(perturbed_output, dim=1)[0, perturbed_pred].item()\n",
        "\n",
        "\n",
        "    # Convert tensors to NumPy arrays for visualization\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    perturbed_image = perturbed_image.detach()\n",
        "\n",
        "\n",
        "    for t, m, s in zip( perturbed_image, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "\n",
        "    perturbed_image = perturbed_image.squeeze().cpu().numpy()\n",
        "\n",
        "\n",
        "    perturbed_image = perturbed_image.clip(0,1).transpose((1, 2, 0))\n",
        "\n",
        "    plt.subplot(PLOT_HEIGHT, PLOT_WIDTH, i + 1)\n",
        "    plt.title(f\"Eps {eps}: Classified as {perturbed_pred.item()} with {confidence:.2f} confidence\")\n",
        "    plt.imshow(perturbed_image)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
